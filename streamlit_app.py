import streamlit as st
import os
import json
from pydantic import BaseModel, Field # Pydantic ë¼ì´ë¸ŒëŸ¬ë¦¬, AgentState ì •ì˜ì— í•„ìˆ˜
from typing import Literal

# LangChain/LangGraph Components
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langgraph.graph.message import AnyMessage, HumanMessage, SystemMessage
from langgraph.checkpoint.sqlite import SqliteSaver

# ==============================================================================
# 1. í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™” (Setup & Initialization)
# ==============================================================================

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (API Key ê´€ë¦¬)
# ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” ë³´ì•ˆì„ ìœ„í•´ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
# Streamlit secrets ë˜ëŠ” OS í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
# ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¥¼ ìœ„í•´ Mockingí•©ë‹ˆë‹¤.
os.environ["OPENAI_API_KEY"] = os.environ.get("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY")

# Firebase ì „ì—­ ë³€ìˆ˜ ì„¤ì • (Canvas í™˜ê²½ í•„ìˆ˜ ìš”ì†Œ)
# ì´ ì˜ˆì œì—ì„œëŠ” Firestoreë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë‚˜, í™˜ê²½ í˜¸í™˜ì„±ì„ ìœ„í•´ ë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
try:
    firebaseConfig = json.loads(__firebase_config)
    appId = __app_id
    initialAuthToken = __initial_auth_token
except NameError:
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ í™˜ê²½ì„ ìœ„í•œ ë”ë¯¸ ê°’
    firebaseConfig = {}
    appId = 'default-app-id'
    initialAuthToken = None
    
# LLM ì´ˆê¸°í™” (GPT-4o-mini ì‚¬ìš©)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

# ==============================================================================
# 2. RAG (Retrieval-Augmented Generation) ì„¤ì •
# ==============================================================================

# 2-1. ì„ì‹œ ì—¬í–‰ì§€ ì§€ì‹ ë°ì´í„° (RAG Source)
TRAVEL_KNOWLEDGE = [
    "íŒŒë¦¬(Paris)ëŠ” ì—í íƒ‘, ë£¨ë¸Œë¥´ ë°•ë¬¼ê´€, ë…¸íŠ¸ë¥´ë‹´ ëŒ€ì„±ë‹¹, ëª½ë§ˆë¥´íŠ¸ ì–¸ë•ìœ¼ë¡œ ìœ ëª…í•˜ë©°, ì˜ˆìˆ ê³¼ ë¯¸ì‹ì˜ ì¤‘ì‹¬ì§€ì…ë‹ˆë‹¤. ì£¼ìš” ë¯¸ì‹ì€ ë§ˆì¹´ë¡±, í¬ë£¨ì•„ìƒ, ì—ìŠ¤ì¹´ë¥´ê³ ì…ë‹ˆë‹¤. í‰ê·  ì˜ˆì‚°ì€ 1ì¼ë‹¹ 150~250 ìœ ë¡œì…ë‹ˆë‹¤.",
    "ì„œìš¸(Seoul)ì€ ê²½ë³µê¶, ë‚¨ì‚°íƒ€ì›Œ, ëª…ë™, í™ëŒ€, ê°•ë‚¨ ë“± ì „í†µê³¼ í˜„ëŒ€ê°€ ê³µì¡´í•˜ëŠ” ë„ì‹œì…ë‹ˆë‹¤. í•œê°•ì—ì„œ ë¼ì´ë”©ì„ ì¦ê¸°ê±°ë‚˜ K-íŒ ì„±ì§€ ìˆœë¡€ë„ ì¸ê¸°ì…ë‹ˆë‹¤. ì£¼ìš” ìŒì‹ì€ ê¹€ì¹˜ì°Œê°œ, ë¹„ë¹”ë°¥, ì¹˜ë§¥ì´ë©°, í‰ê·  ì˜ˆì‚°ì€ 1ì¼ë‹¹ 10ë§Œ~15ë§Œ ì›ì…ë‹ˆë‹¤.",
    "ë„ì¿„(Tokyo)ëŠ” ì‹ ì£¼ì¿ , ì‹œë¶€ì•¼, ì•„ì‚¬ì¿ ì‚¬ ë“± ë‹¤ì–‘í•œ ë§¤ë ¥ì„ ì§€ë‹ˆê³  ìˆìŠµë‹ˆë‹¤. ì˜¤íƒ€ì¿  ë¬¸í™”ì˜ ì•„í‚¤í•˜ë°”ë¼ì™€ ë¯¸ìŠë­ ë ˆìŠ¤í† ë‘ì´ ë§ìŠµë‹ˆë‹¤. ì£¼ìš” ë¯¸ì‹ì€ ìŠ¤ì‹œ, ë¼ë©˜, í…í‘¸ë¼ì…ë‹ˆë‹¤. í‰ê·  ì˜ˆì‚°ì€ 1ì¼ë‹¹ 15,000~25,000 ì—”ì…ë‹ˆë‹¤.",
    "ì—¬í–‰ì§€ ì„ ì • ì‹œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” ì˜ˆì‚°ê³¼ ê¸°ê°„ì…ë‹ˆë‹¤. ì˜ˆì‚°ì´ ì œí•œì ì´ë¼ë©´ ë™ë‚¨ì•„ì‹œì•„ ì§€ì—­ì´ë‚˜ êµ­ë‚´ ì—¬í–‰ì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¢‹ê³ , ê¸°ê°„ì´ ê¸¸ë‹¤ë©´ ìœ ëŸ½ì´ë‚˜ ë¯¸ì£¼ ì§€ì—­ì„ ì¶”ì²œí•©ë‹ˆë‹¤. í•­ìƒ í˜„ì§€ ë‚ ì”¨ë¥¼ í™•ì¸í•˜ì„¸ìš”."
]

# 2-2. Vector Store ìƒì„± (FAISS í™œìš©)
@st.cache_resource
def setup_vector_store():
    """RAGì— ì‚¬ìš©í•  Vector Storeë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
    # Document ê°ì²´ ìƒì„±
    docs = [Document(page_content=t) for t in TRAVEL_KNOWLEDGE]
    # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    # FAISS Vector Store ìƒì„± ë° ì¸ë±ì‹±
    vectorstore = FAISS.from_documents(docs, embeddings)
    return vectorstore

vectorstore = setup_vector_store()
retriever = vectorstore.as_retriever()

# RAG Chain ì •ì˜
retrieval_chain = create_retrieval_chain(
    retriever,
    create_stuff_documents_chain(
        llm,
        ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì„¸ê³„ì ì¸ ì—¬í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ 'ê²€ìƒ‰ëœ ì •ë³´'ì™€ 'ì‚¬ìš©ìì˜ ì—¬í–‰ ìš”ì²­'ì„ ë°”íƒ•ìœ¼ë¡œ ì—¬í–‰ ê³„íš ìˆ˜ë¦½ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì„¸ìš”. ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ì„ ê²½ìš°, ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."),
            ("context", "{context}"),
            ("human", "ì‚¬ìš©ìì˜ ì—¬í–‰ ìš”ì²­: {input}"),
        ])
    )
)

# ==============================================================================
# 3. LangGraph & Multi-Agent Flow ì •ì˜ (Planner + Tool)
# ==============================================================================

# 3-1. Tool ì •ì˜ (ReActë¥¼ ìœ„í•œ RAG ê¸°ëŠ¥ ë„êµ¬)
@tool
def research_travel_info(query: str) -> str:
    """
    ì—¬í–‰ ê³„íšì— í•„ìš”í•œ íŠ¹ì • ë„ì‹œ, ìŒì‹, ëª…ì†Œ, ì˜ˆì‚° ë“±ì˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì˜ˆ: 'íŒŒë¦¬ì˜ ì£¼ìš” ëª…ì†ŒëŠ” ë­ì•¼?'
    """
    st.session_state.messages.append(SystemMessage(content=f"ğŸ¤” **ë¦¬ì„œì¹˜ ë„êµ¬ ì‚¬ìš©:** '{query}'ì— ëŒ€í•œ ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."))
    
    # RAG Chain ì‹¤í–‰
    result = retrieval_chain.invoke({"input": query})
    
    # ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì„¸ì…˜ì— ì¶”ê°€
    retrieved_context = result['context']
    
    # ê²€ìƒ‰ëœ ì •ë³´ê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
    if not retrieved_context:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."

    # contextë¥¼ ë¬¸ìì—´ë¡œ í•©ì¹˜ê³ , ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜
    context_str = "\n".join([doc.page_content for doc in retrieved_context])

    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í™œìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” LLM í˜¸ì¶œ
    planner_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ 'ê²€ìƒ‰ëœ ë¬¸ì„œë“¤'ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”ì²­ì— ëŒ€í•´ í•„ìš”í•œ í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì—¬ ì œê³µí•˜ì„¸ìš”."),
        ("human", f"ê²€ìƒ‰ ìš”ì²­: {query}\n\nê²€ìƒ‰ëœ ë¬¸ì„œë“¤:\n{context_str}"),
    ])
    
    # LLMì´ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ìš”ì•½í•˜ë„ë¡ í•œ ë²ˆ ë” í˜¸ì¶œ
    summary = llm.invoke(planner_prompt).content
    st.session_state.messages.append(SystemMessage(content=f"âœ… **ë¦¬ì„œì¹˜ ê²°ê³¼:** {summary}"))
    return summary


# 3-2. Agent State ì •ì˜
class AgentState(BaseModel):
    """
    ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤. Pydanticì„ ì‚¬ìš©í•˜ì—¬ ìƒíƒœ ê´€ë¦¬ë¥¼ ëª…í™•í•˜ê²Œ í•©ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­
    initial_request: str = Field(description="ì‚¬ìš©ìì˜ ìµœì´ˆ ì—¬í–‰ ê³„íš ìš”ì²­")
    # ëŒ€í™” ê¸°ë¡ (ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory)
    chat_history: list[AnyMessage] = Field(description="í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸")
    # ì—ì´ì „íŠ¸ê°€ ìˆ˜ì§‘í•œ í•µì‹¬ ì •ë³´
    contextual_data: str = Field(description="ë¦¬ì„œì¹˜ì™€ ëŒ€í™”ë¥¼ í†µí•´ ìˆ˜ì§‘ëœ í•µì‹¬ ì—¬í–‰ ì •ë³´")
    # LangGraphì—ì„œ ì‚¬ìš©í•  ë‹¤ìŒ ì•¡ì…˜
    next_action: Literal["tool_call", "generate_itinerary", "final_answer"] = Field(description="ë‹¤ìŒ ìˆ˜í–‰í•  ì•¡ì…˜")
    # ReAct Tool í˜¸ì¶œ ì‹œ ì‚¬ìš©í•  ì¸ë³´ì¼€ì´ì…˜ (LangGraph prebuilt ToolExecutorìš©)
    tool_invocation: ToolInvocation | None = Field(description="í˜¸ì¶œí•  ë„êµ¬ ë° ì¸ì")

# 3-3. Agent Nodes ì •ì˜

# A. í”Œë˜ë„ˆ ë…¸ë“œ (Planner Node - ReAct Logic)
def planner_agent(state: AgentState):
    """
    ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³ , ë¦¬ì„œì¹˜(Tool)ê°€ í•„ìš”í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    st.session_state.messages.append(SystemMessage(content="ğŸ§  **í”Œë˜ë„ˆ ì—ì´ì „íŠ¸:** ìš”ì²­ ë¶„ì„ ë° ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."))
    
    # LangChain Runnableì„ ì‚¬ìš©í•˜ì—¬ ReAct ë¡œì§ êµ¬í˜„
    prompt = ChatPromptTemplate.from_messages([
        # Few-shot Prompting ë° Role-playing
        ("system", """
         ë‹¹ì‹ ì€ ì—¬í–‰ ê³„íš ìˆ˜ë¦½ì„ ìœ„í•œ ìµœê³  ìˆ˜ì¤€ì˜ í”Œë˜ë‹ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
         ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬, ì—¬í–‰ ê³„íšì„ ìˆ˜ë¦½í•˜ê¸° ì „ì— 'research_travel_info' ë„êµ¬ì˜ ì‚¬ìš©ì´ í•„ìš”í•œì§€ ê²°ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
         
         1. ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´, ì ì ˆí•œ 'tool_call'ì„ ìƒì„±í•˜ì—¬ ë‹¤ìŒ ìƒíƒœë¡œ ë„˜ê¹ë‹ˆë‹¤. ë„êµ¬ì— ì œê³µí•  ì¸ìëŠ” ë§¤ìš° êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
         2. ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•˜ì§€ ì•Šê±°ë‚˜, ì´ë¯¸ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆë‹¤ê³  íŒë‹¨ë˜ë©´, ë‹¤ìŒ ë‹¨ê³„ì¸ 'generate_itinerary'ë¡œ ë„˜ê¹ë‹ˆë‹¤.

         **ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:** research_travel_info(query: str)
         
         **Chain-of-Thought ì˜ˆì‹œ:**
         ìš”ì²­: 2ë°• 3ì¼ ì„œìš¸ ì—¬í–‰ ê³„íšì„ ì§œì¤˜.
         Thought: ì„œìš¸ì˜ ì£¼ìš” ëª…ì†Œì™€ ì˜ˆì‚° ì •ë³´ê°€ í•„ìš”í•˜ë‹¤.
         Action: research_travel_info(query='ì„œìš¸ì˜ ì£¼ìš” ëª…ì†Œ, ìŒì‹, ì˜ˆìƒ ê²½ë¹„')
         
         **Output í˜•ì‹ (ë°˜ë“œì‹œ JSONìœ¼ë¡œ ì‘ë‹µ):**
         {{ "next_action": "tool_call" | "generate_itinerary", "tool_invocation": ToolInvocation | null, "thought": "ë‹¹ì‹ ì˜ ì¶”ë¡  ê³¼ì •" }}
         """),
        ("human", f"ì‚¬ìš©ìì˜ ìš”ì²­: {state.initial_request}"),
    ]).partial(tools=[research_travel_info])
    
    # LLM í˜¸ì¶œ ë° JSON ì‘ë‹µ íŒŒì‹±
    parser = JsonOutputParser(pydantic_object=AgentState)
    
    # CoTë¥¼ ìœ ë„í•˜ê¸° ìœ„í•´ `response_format`ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  LLMì—ê²Œ JSON ì¶œë ¥ì„ ìš”ì²­
    response = llm.invoke(prompt)
    
    try:
        # LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ì„ ì¶”ì¶œí•˜ì—¬ íŒŒì‹± ì‹œë„
        # LangChainì˜ PydanticOutputParserê°€ ìë™ ë³€í™˜ì„ ì‹œë„í•˜ì§€ë§Œ,
        # LLMì´ ë¬¸ìì—´ ì•ˆì— JSONì„ ë„£ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì§ì ‘ íŒŒì‹± ì‹œë„
        json_content = response.content.strip().split("```json")[1].split("```")[0].strip()
        parsed_data = json.loads(json_content)
        
        # Pydantic ëª¨ë¸ì— ë§ê²Œ ë°ì´í„° ì •ë¦¬
        tool_invocation = None
        if parsed_data.get("tool_invocation"):
            # ToolInvocation ê°ì²´ë¡œ ë³€í™˜
            inv = parsed_data["tool_invocation"]
            tool_invocation = ToolInvocation(
                tool=inv["tool"], 
                tool_input=inv["tool_input"], 
                id=inv.get("id")
            )
        
        # ìƒíƒœ ì—…ë°ì´íŠ¸
        new_state = state.copy(update={
            "next_action": parsed_data.get("next_action", "generate_itinerary"), # ê¸°ë³¸ê°’ ì„¤ì •
            "tool_invocation": tool_invocation,
        })
        return new_state
        
    except Exception as e:
        st.session_state.messages.append(SystemMessage(content=f"âš ï¸ **JSON íŒŒì‹± ì˜¤ë¥˜. ê°•ì œ ì§„í–‰:** {e}"))
        # ì˜¤ë¥˜ ì‹œ ê°•ì œë¡œ ì¼ì • ìƒì„± ë‹¨ê³„ë¡œ ì´ë™
        return state.copy(update={"next_action": "generate_itinerary"})

# B. ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ (Tool Execution Node)
def execute_tool(state: AgentState):
    """LangGraph Prebuilt ToolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    tool_executor = ToolExecutor([research_travel_info])
    
    if state.tool_invocation:
        # ë„êµ¬ ì‹¤í–‰
        output = tool_executor.invoke(state.tool_invocation)
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        new_context = state.contextual_data + "\n\n[ê²€ìƒ‰ ê²°ê³¼]\n" + str(output)
        
        # ë„êµ¬ë¥¼ ì‹¤í–‰í–ˆìœ¼ë‹ˆ, ë‹¤ìŒì€ ì¼ì • ìƒì„±ìœ¼ë¡œ ë„˜ì–´ê°€ë„ë¡ ìƒíƒœ ì—…ë°ì´íŠ¸
        return state.copy(update={
            "contextual_data": new_context,
            "next_action": "generate_itinerary" # ë¦¬ì„œì¹˜ í›„ í•­ìƒ ì¼ì • ìƒì„±ìœ¼ë¡œ ì´ë™
        })
    
    # ë„êµ¬ í˜¸ì¶œ ì •ë³´ê°€ ì—†ìœ¼ë©´, í”Œë˜ë„ˆê°€ ì˜ëª»ëœ ìƒíƒœë¥¼ ë„˜ê²¼ìœ¼ë¯€ë¡œ ê°•ì œ ì§„í–‰
    return state.copy(update={"next_action": "generate_itinerary"})


# C. ì¼ì • ìƒì„± ë…¸ë“œ (Itinerary Generation Node)
def itinerary_generator(state: AgentState):
    """
    ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì—¬í–‰ ì¼ì •ì„ ìƒì„±í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•©ë‹ˆë‹¤.
    """
    st.session_state.messages.append(SystemMessage(content="âœï¸ **ì¼ì • ìƒì„± ì—ì´ì „íŠ¸:** ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì¼ì •ì„ ì‘ì„±í•©ë‹ˆë‹¤."))
    
    # í”„ë¡¬í”„íŠ¸ ìµœì í™”: ì—­í•  ë¶€ì—¬ + ìˆ˜ì§‘ëœ ì •ë³´ í™œìš©
    generator_prompt = ChatPromptTemplate.from_messages([
        ("system", """
         ë‹¹ì‹ ì€ ì „ë¬¸ ì—¬í–‰ ê°€ì´ë“œì´ì ì¼ì • ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
         ì•„ë˜ 'ì‚¬ìš©ì ìš”ì²­'ê³¼ 'ìˆ˜ì§‘ëœ ì •ë³´'ë¥¼ ì¢…í•©í•˜ì—¬, **ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìƒì„¸í•œ ì—¬í–‰ ì¼ì •ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.**
         
         ì¼ì •ì€ ì¼ìë³„ë¡œ(ì˜ˆ: 1ì¼ì°¨, 2ì¼ì°¨) êµ¬ë¶„í•˜ê³ , ê° í™œë™ì— ëŒ€í•´ ê°„ê²°í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì˜ˆìƒ ì‹œê°„, ì¥ì†Œ, í•µì‹¬ íŒ(ì˜ˆ: ì˜ˆì‚°, êµí†µ ë“±)ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
         ìˆ˜ì§‘ëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„, ì¼ë°˜ì ì¸ ì—¬í–‰ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì™„ë²½í•œ ì¼ì •ì„ ì™„ì„±í•˜ì„¸ìš”.
         """),
        ("context", f"**ìˆ˜ì§‘ëœ ì •ë³´:**\n{state.contextual_data}"),
        ("human", f"**ì‚¬ìš©ì ìš”ì²­:**\n{state.initial_request}"),
    ])
    
    # LLM í˜¸ì¶œ
    itinerary_response = llm.invoke(generator_prompt).content
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸: ìµœì¢… ë‹µë³€ì„ chat_historyì— ì¶”ê°€í•˜ê³  flowë¥¼ ì¢…ë£Œ
    new_history = state.chat_history + [HumanMessage(content=state.initial_request), SystemMessage(content=itinerary_response)]
    return state.copy(update={
        "chat_history": new_history,
        "next_action": "final_answer", # ìµœì¢… ë‹µë³€ ì™„ë£Œ
    })

# 3-4. LangGraph Flow êµ¬ì„±
def build_graph():
    """LangGraph ìƒíƒœ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    # Graph ì´ˆê¸°í™”
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("planner", planner_agent)
    workflow.add_node("execute_tool", execute_tool)
    workflow.add_node("itinerary_generator", itinerary_generator)

    # ì—£ì§€(Edge) ì •ì˜: ë…¸ë“œ ê°„ ì´ë™ ê²½ë¡œ
    # 1. ì‹œì‘ -> í”Œë˜ë„ˆ
    workflow.add_edge(START, "planner")

    # 2. í”Œë˜ë„ˆ ë…¸ë“œì˜ ê²°ê³¼ì— ë”°ë¼ ì´ë™
    def route_planner(state: AgentState):
        if state.next_action == "tool_call":
            return "execute_tool"
        elif state.next_action == "generate_itinerary":
            return "itinerary_generator"
        else:
            return END # ì˜ˆì™¸ ìƒí™© ëŒ€ë¹„
    
    workflow.add_conditional_edges(
        "planner", 
        route_planner
    )
    
    # 3. ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ -> ì¼ì • ìƒì„± ë…¸ë“œ (ë¦¬ì„œì¹˜ í›„ì—ëŠ” í•­ìƒ ì¼ì • ìƒì„±)
    workflow.add_edge("execute_tool", "itinerary_generator")
    
    # 4. ì¼ì • ìƒì„± ë…¸ë“œ -> ì¢…ë£Œ (ìµœì¢… ë‹µë³€ ì™„ë£Œ)
    workflow.add_edge("itinerary_generator", END)
    
    # ë©”ëª¨ë¦¬ ì €ì¥ì„ ìœ„í•œ Checkpoint ì„¤ì • (ë©€í‹°í„´ ëŒ€í™”/ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì„ íƒ ì‚¬í•­)
    memory = SqliteSaver.from_conn_string(":memory:")
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app = workflow.compile(checkpointer=memory)
    return app

# Graph ì»´íŒŒì¼ (Streamlit ë¦¬ì†ŒìŠ¤ ìºì‹œ í™œìš©)
@st.cache_resource
def get_graph():
    return build_graph()

graph_app = get_graph()

# ==============================================================================
# 4. Streamlit UI (ì„œë¹„ìŠ¤ ê°œë°œ)
# ==============================================================================

st.set_page_config(
    page_title="AI ì—¬í–‰ í”Œë˜ë„ˆ âœˆï¸ (LangChain/RAG/Streamlit)",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì‚¬ìš©ìì—ê²Œ LLM í˜¸ì¶œ ë° RAG êµ¬ì„±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
with st.sidebar:
    st.header("ê³¼ì œ ìš”êµ¬ì‚¬í•­ êµ¬í˜„")
    st.markdown("""
    **âœ… êµ¬í˜„ ìš”ì†Œ:**
    - **Streamlit**: ëŒ€í™”í˜• UI êµ¬í˜„
    - **LangChain/LangGraph**: Multi-Agent Flow ì„¤ê³„
    - **RAG**: ë‚´ì¥ ì§€ì‹(FAISS) ê¸°ë°˜ ì •ë³´ ê²€ìƒ‰
    - **Prompt Engineering**: ì—­í•  ë¶€ì—¬, CoT, Few-shot í™œìš©
    - **ReAct (Tool)**: RAG ê²€ìƒ‰ ê¸°ëŠ¥ì„ ë„êµ¬ë¡œ í™œìš©
    - **API Key ê´€ë¦¬**: í™˜ê²½ ë³€ìˆ˜ (Mock)
    """)
    st.info("ğŸ’¡ **ì‚¬ìš©ë²•:** ì—¬í–‰ì§€, ê¸°ê°„, ì˜ˆì‚°, ê´€ì‹¬ì‚¬ë¥¼ í¬í•¨í•˜ì—¬ ìƒì„¸í•œ ìš”ì²­ì„ ì…ë ¥í•˜ì„¸ìš”.")
    
    # Chat History ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ìƒˆë¡œìš´ ì—¬í–‰ ê³„íš ì‹œì‘", use_container_width=True):
        st.session_state.messages = [SystemMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ì—¬í–‰ í”Œë˜ë„ˆì…ë‹ˆë‹¤. ì–´ë–¤ ì—¬í–‰ ê³„íšì„ ë„ì™€ë“œë¦´ê¹Œìš”? (ì˜ˆ: 3ë°• 4ì¼ íŒŒë¦¬ ê°€ì¡± ì—¬í–‰, ì˜ˆì‚° 500ë§Œì›)")]
        st.session_state.run_id = None
        st.session_state.initial_request = None
        st.success("ìƒˆë¡œìš´ ëŒ€í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")


st.title("AI ì—¬í–‰ í”Œë˜ë„ˆ âœˆï¸")
st.caption("ìš”êµ¬ì‚¬í•­ì— ë§ì¶° LangGraph Multi-Agent, RAG, Streamlitìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [SystemMessage(content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ì—¬í–‰ í”Œë˜ë„ˆì…ë‹ˆë‹¤. ì–´ë–¤ ì—¬í–‰ ê³„íšì„ ë„ì™€ë“œë¦´ê¹Œìš”? (ì˜ˆ: 3ë°• 4ì¼ íŒŒë¦¬ ê°€ì¡± ì—¬í–‰, ì˜ˆì‚° 500ë§Œì›)")]
if "run_id" not in st.session_state:
    st.session_state.run_id = None
if "initial_request" not in st.session_state:
    st.session_state.initial_request = None


# ì´ì „ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    if isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    elif isinstance(message, SystemMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)
    # LangGraph ë‚´ë¶€ ë©”ì‹œì§€ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ (ì„ íƒì ìœ¼ë¡œ í‘œì‹œ ê°€ëŠ¥)


# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì—¬í–‰ ê³„íšì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì‘ë‹µì„ ìœ„í•œ Chat Assistant ì˜ì—­
    with st.chat_message("assistant"):
        st.session_state.messages.append(SystemMessage(content="ì—¬í–‰ ê³„íš ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."))
        # ë¡œë”© ìŠ¤í”¼ë„ˆ
        with st.spinner("ìµœê³ ì˜ ì—¬í–‰ ì¼ì •ì„ ë§Œë“¤ê¸° ìœ„í•´ AI Agentë“¤ì´ í˜‘ë ¥ ì¤‘ì…ë‹ˆë‹¤..."):
            
            # LangGraph ì‹¤í–‰ì„ ìœ„í•œ ì´ˆê¸° ìƒíƒœ
            initial_state = AgentState(
                initial_request=prompt,
                chat_history=[],
                contextual_data="",
                next_action="planner",
                tool_invocation=None
            )
            
            # LangGraph ì‹¤í–‰ (stream ì‚¬ìš©)
            # ì—¬ê¸°ì„œëŠ” state.chat_historyì— ìµœì¢… ë‹µë³€ë§Œ ë‹´ê¸° ë•Œë¬¸ì— ìŠ¤íŠ¸ë¦¬ë°ì€ ë‹¨ìˆœ ë¡œë”© íš¨ê³¼ìš©ì…ë‹ˆë‹¤.
            # ë³µì¡í•œ ë©€í‹°í„´ ëŒ€í™”ë‚˜ ìƒíƒœ ì €ì¥/ë³µêµ¬ ê¸°ëŠ¥ì€ `checkpointer`ê°€ ë‹´ë‹¹í•©ë‹ˆë‹¤.
            
            final_state = None
            try:
                # ê·¸ë˜í”„ ì‹¤í–‰
                for s in graph_app.stream(initial_state, config={"configurable": {"thread_id": "my_travel_plan"}}):
                    # ìƒíƒœ ë³€í™”ë¥¼ ê°ì§€í•˜ê³  ìµœì¢… ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
                    final_state = s
                    
                # ìµœì¢… ìƒíƒœì—ì„œ ìƒì„±ëœ ë‹µë³€ ì¶”ì¶œ
                if final_state and "itinerary_generator" in final_state:
                    # itinerary_generator ë…¸ë“œê°€ ìµœì¢… ìƒíƒœì— ë„ë‹¬í–ˆì„ ë•Œì˜ chat_historyë¥¼ ì‚¬ìš©
                    response_message = final_state["itinerary_generator"].chat_history[-1].content
                    
                    # ìµœì¢… ë‹µë³€ í‘œì‹œ ë° ì„¸ì…˜ ì—…ë°ì´íŠ¸
                    st.markdown(response_message)
                    st.session_state.messages = st.session_state.messages[:-1] + [SystemMessage(content=response_message)]
                else:
                    error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ì—¬í–‰ ì¼ì • ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìš”ì²­ì„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                    st.markdown(error_message)
                    st.session_state.messages = st.session_state.messages[:-1] + [SystemMessage(content=error_message)]

            except Exception as e:
                error_message = f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_message)
                st.session_state.messages = st.session_state.messages[:-1] + [SystemMessage(content=error_message)]
